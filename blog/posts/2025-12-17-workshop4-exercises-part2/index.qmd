---
title: "Workshop 4 Preview: Vision Alone - When the Camera Goes Blind"
subtitle: "Part 2 of 4: Why Visual Odometry Fails and What IMU Can't See"
author: "Rajesh"
date: "2025-12-17T13:00:00"
categories: [ros2, visual-odometry, perception, workshop, roscon-india, realsense, d435i, rtabmap]
image: "thumbnail.png"
toc: true
toc-depth: 3
code-fold: true
code-summary: "Show code"
lightbox: true
---

## The Story So Far

In [Part 1](../2025-12-17-workshop4-exercises-part1/), we experienced the limitations of IMU-only sensing:

| Problem | Impact |
|---------|--------|
| No raw orientation | Need Madgwick filter |
| Yaw drift (no magnetometer) | Unbounded heading error |
| Position drift | Meters of error in seconds |
| Calibration required | Extra setup work |

**The conclusion**: IMU alone isn't enough for robot navigation.

But wait - the D435i has **cameras** too! Can visual odometry solve these problems?

---

## What This Part Covers

Now we'll try **vision-only** approaches and discover their own failure modes.

| Experiment | Test | Problem Discovered |
|------------|------|-------------------|
| **8** | Visual Odometry | Fast motion = tracking loss |
| **9** | Textureless Surfaces | No features = no tracking |
| **10** | Lighting Changes | Exposure changes = drift |

::: {.callout-important}
## The Key Insight Coming
IMU fails where vision succeeds, and vision fails where IMU succeeds. This is why **fusion** (Part 3) is the answer!
:::

---

## Setting Up Visual Odometry

### Launch RTAB-Map Visual Odometry

RTAB-Map includes a powerful visual odometry module that works with RGB-D cameras like the D435i.

![RTAB-Map visual odometry workflow: RGB + Depth â†’ Feature Extraction â†’ Pose Estimation](rtabmap-visual-odometry.png){.lightbox fig-alt="RTAB-Map visual odometry pipeline diagram"}

```bash
# Terminal 1: Launch RealSense camera
ros2 launch realsense2_camera rs_launch.py \
    enable_gyro:=true \
    enable_accel:=true \
    unite_imu_method:=2 \
    align_depth.enable:=true

# Terminal 2: Launch RTAB-Map visual odometry ONLY (no IMU yet!)
ros2 launch rtabmap_launch rtabmap.launch.py \
    args:="--delete_db_on_start" \
    rgb_topic:=/camera/camera/color/image_raw \
    depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
    camera_info_topic:=/camera/camera/color/camera_info \
    frame_id:=camera_link \
    approx_sync:=true \
    visual_odometry:=true \
    imu_topic:=""
```

### Verify It's Working

```bash
# Terminal 3: Check odometry output
ros2 topic echo /rtabmap/odom --field pose.pose.position

# Terminal 4: Check odometry status
ros2 topic echo /rtabmap/odom_info --field lost --field features --field inliers

# Terminal 5: Launch RViz2
rviz2
# Add: TF, PointCloud2 (/rtabmap/cloud_map), Odometry (/rtabmap/odom)
```

**When working (feature-rich scene)**, you should see stable tracking:

```
Checking VO status (camera pointed at textured bookshelf)...
lost: false matches: 538 inliers: 114 features: 907
lost: false matches: 541 inliers: 127 features: 905
lost: false matches: 560 inliers: 123 features: 907
```

Key metrics to watch:
- `lost: false` = tracking is working
- `features: ~900` = good feature detection
- `inliers: >20` = good geometric consistency (minimum: 20)

---

## Experiment 8: Fast Motion = Lost Tracking

### What You'll Experience

Visual odometry relies on **feature matching** between consecutive frames. When motion is too fast, features blur and matching fails!

### The Test

```bash
# Monitor odometry while testing
ros2 topic hz /odom
ros2 topic echo /odom --field pose.pose.position
```

### Procedure

1. Hold camera steady - observe stable tracking
2. Move camera slowly - observe smooth odometry
3. **Shake camera rapidly** for 2 seconds
4. Stop and observe

### What Happens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VISUAL ODOMETRY vs FAST MOTION                        â”‚
â”‚                                                                         â”‚
â”‚    Frame N          Frame N+1 (motion blur)      Frame N+2             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚ â˜…  â˜…    â”‚      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚         â”‚    â˜…  â˜… â”‚           â”‚
â”‚    â”‚    â˜…    â”‚  â”€â”€â–º â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚  â”€â”€â–º   â”‚  â˜…      â”‚           â”‚
â”‚    â”‚  â˜…   â˜…  â”‚      â”‚ ~~~~~~~~~~~~~~~~ â”‚         â”‚     â˜…  â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚    Clear features   Motion blur!                 New features          â”‚
â”‚                     No matches! ðŸ”´                Can't match to N!    â”‚
â”‚                                                                         â”‚
â”‚    Result: Odometry JUMPS or FAILS completely!                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![Motion blur destroys feature matching - camera shake causes tracking loss](motion-blur-tracking-loss.png){.lightbox fig-alt="Motion blur causing visual odometry failure across three frames"}

### Actual Console Output - Baseline Test

**Stable tracking (slow motion, textured scene):**

We captured 10 seconds of data while slowly panning the camera across a bookshelf:

```
==============================================================
BASELINE: Visual Odometry with Good Scene
Camera pointed at feature-rich scene
==============================================================

Time     Lost    Features  Matches  Inliers   Status
------------------------------------------------------
   1s    false        905      540      107   OK
   2s    false        911      539      129   OK
   3s    false        904      557      127   OK
   4s    false        903      552      128   OK
   5s    false        916      512      137   OK
   6s    false        900      552      128   OK
   7s    false        908      525      121   OK
   8s    false        899      546      117   OK
   9s    false        907      532      124   OK
  10s    false        906      546      119   OK
------------------------------------------------------

ANALYSIS: With feature-rich scene, tracking is stable!
  - High feature count (~900)
  - Many inliers (>100, well above 20 minimum)
  - No tracking loss
```

**Fast shake test** - when you shake the camera rapidly:

```
Time     Lost    Features  Matches  Inliers   Status
------------------------------------------------------
   5s    false        904      518      115   OK      <- Stable before shake
   6s    false        918      519      109   OK
   7s    true         412       18        0   LOST!   <- Shaking starts
   8s    true         387       12        0   LOST!   <- Motion blur
   9s    true         445       21        0   LOST!   <- Can't match features
  10s    false        892      498       87   OK      <- Recovered after stopping
------------------------------------------------------
```

### RTAB-Map Warning Messages

Watch the terminal for these warnings during fast motion:

```
[WARN] (OdometryF2M.cpp:622) Registration failed: "Not enough inliers 0/20
       (matches=18) between -1 and 1061"
[WARN] (OdometryF2M.cpp:622) Registration failed: "Not enough inliers 0/20
       (matches=12) between -1 and 1062"
[ERROR] (Rtabmap.cpp:1408) RGB-D SLAM mode is enabled, memory is incremental
        but no odometry is provided. Image 0 is ignored!
```

The `0/20` means 0 inliers out of the 20 required minimum!

### Eureka Moment #8

::: {.callout-tip}
## Vision Fails on Fast Motion

**Why it fails:**
1. Camera captures at 30 FPS = 33ms between frames
2. Fast motion = large displacement between frames
3. Motion blur destroys features
4. Feature matching fails â†’ tracking lost

**What IMU provides:**
- IMU runs at 400 Hz = 2.5ms between samples
- IMU is immune to motion blur
- IMU can "predict" camera pose during blur

This is exactly what fusion solves in Part 3!
:::

---

## Experiment 9: Textureless Surfaces = No Features

### What You'll Experience

Visual odometry needs **visual features** (corners, edges, textures). Point the camera at a blank wall and watch it fail!

### The Test

Point the D435i at different surfaces:

1. **Textured surface** (bookshelf, posters) - should work
2. **Plain white wall** - expect failure
3. **Uniform floor/ceiling** - expect failure

### Procedure

```bash
# Monitor feature count (if RTAB-Map exposes it)
ros2 topic echo /rtabmap/info --field word_count

# Or watch for warnings
# Terminal running rtabmap_launch will show warnings
```

### What Happens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FEATURE DETECTION vs SURFACE TEXTURE                  â”‚
â”‚                                                                         â”‚
â”‚    Textured Scene                    Textureless Scene                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ â˜…  ðŸ“š  â˜…  ðŸ–¼ï¸  â”‚              â”‚                 â”‚                â”‚
â”‚    â”‚    â˜…     â˜…    â”‚              â”‚                 â”‚                â”‚
â”‚    â”‚  ðŸ“·  â˜…   â˜… ðŸª´ â”‚              â”‚                 â”‚                â”‚
â”‚    â”‚ â˜…    â˜…     â˜…  â”‚              â”‚                 â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚    Features: 150+ âœ…                Features: 3 âŒ                     â”‚
â”‚    Tracking: STABLE                  Tracking: FAILS                   â”‚
â”‚                                                                         â”‚
â”‚    Visual odometry needs features to track!                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

::: {layout-ncol=2}
![Textured vs textureless: same camera, dramatically different tracking results](textured-vs-textureless.png){.lightbox group="textureless" fig-alt="Comparison of feature detection on textured vs textureless surfaces"}

![Feature matching geometry: inliers (green) pass validation, outliers (red) are rejected](feature-matching-geometry.png){.lightbox group="textureless" fig-alt="Feature matching with inliers and outliers visualization"}
:::

### Actual Console Output - Textureless Surface Test

**Real test (December 17, 2025)**: We transitioned from a textured scene to a blank white wall:

```
==============================================================
EXPERIMENT 9: TEXTURELESS SURFACE TEST
==============================================================

>>> Camera transitioned from bookshelf to blank white wall

Time     Lost    Features  Matches  Inliers   Status
------------------------------------------------------
   1s    false        907      527      135   OK      <- Textured scene
   2s    false        906      537      122   OK
   3s    false        905      560      111   OK
   4s    false        904      515      123   OK
   ... (camera rotating toward wall) ...
  13s    false        912      506      137   OK      <- Still seeing some texture
  14s    true         819       37        0   LOST!   <- Now facing wall!
  15s    true         793       21        0   LOST!   <- No features to track
------------------------------------------------------

ANALYSIS:
  - Frames with tracking LOST: 2 / 15
  - Notice: Features dropped from 912 â†’ 819 â†’ 793
  - Inliers dropped from 137 â†’ 0 â†’ 0
```

**Extended test** - continuing to face the blank wall:

```
Time     Lost    Features  Matches  Inliers   Status
------------------------------------------------------
   1s    true         859       11        0   LOST!
   2s    true         801       36        0   LOST!
   3s    true         826       38        0   LOST!
   4s    true         739       29        0   LOST!
   5s    true         816       28        0   LOST!
   6s    true         858       20        0   LOST!
   7s    true         870       41        0   LOST!
   8s    true         864       43        0   LOST!
   9s    true         836       43        0   LOST!
------------------------------------------------------

SUMMARY:
  - Tracking LOST: 9 / 9 frames (100%!)
  - Even with ~800 features detected, 0 inliers!
  - Blank walls have features (edges, gradients) but no MATCHABLE structure
```

### RTAB-Map Warning Messages

```
[WARN] (OdometryF2M.cpp:622) Registration failed: "Not enough inliers 0/20
       (matches=37) between -1 and 1061"
[WARN] (OdometryF2M.cpp:622) Registration failed: "Not enough inliers 0/20
       (matches=21) between -1 and 1062"
```

**Key insight**: Even with matches=37, inliers=0. The features detected on a blank wall are not geometrically consistent - they're noise, not real structure!

### Real-World Failure Scenarios

| Environment | Features | Matches | Inliers | Tracking |
|-------------|----------|---------|---------|----------|
| Bookshelf | ~900 | ~540 | ~120 | âœ… Excellent |
| Desk with objects | ~850 | ~480 | ~100 | âœ… Good |
| Textured wall (posters) | ~600 | ~350 | ~80 | âœ… Good |
| Plain painted wall | ~800 | ~30 | **0** | âŒ FAILS |
| White ceiling | ~700 | ~25 | **0** | âŒ FAILS |
| Looking at floor | ~500 | ~40 | **0** | âŒ FAILS |

### Eureka Moment #9

::: {.callout-tip}
## Vision Needs Visual Features

**Why it fails:**
- Feature detectors (ORB, SIFT) need corners/edges
- Blank walls have no distinctive points
- Can't match what doesn't exist!

**What IMU provides:**
- IMU measures motion directly from physics
- Works regardless of what camera sees
- Can track through textureless regions

**Real robot scenarios:**
- Warehouse: long plain corridors
- Hospital: white walls everywhere
- Factory: uniform floors
- Outside: blue sky, open fields
:::

---

## Experiment 10: Lighting Changes = Drift

### What You'll Experience

Visual features depend on pixel values. Lighting changes alter those values, causing feature matching to degrade.

### The Test

```bash
# Start with normal room lighting
# Monitor odometry position

# Then:
# 1. Turn lights off
# 2. Walk toward/away from window
# 3. Point camera at light source then away
```

### What Happens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LIGHTING CHANGES vs VISUAL ODOMETRY                   â”‚
â”‚                                                                         â”‚
â”‚    Before (normal light)           After (auto-exposure change)        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ â˜…(200,200,200)  â”‚            â”‚ â˜…(100,100,100)  â”‚                  â”‚
â”‚    â”‚    â˜…(180,180,180)â”‚   â”€â”€â–º    â”‚    â˜…(90,90,90)  â”‚                   â”‚
â”‚    â”‚ â˜…(210,210,210)  â”‚            â”‚ â˜…(105,105,105)  â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                         â”‚
â”‚    Same features, different pixel values!                              â”‚
â”‚    Feature descriptor matching becomes unreliable.                     â”‚
â”‚                                                                         â”‚
â”‚    Results:                                                            â”‚
â”‚    â€¢ Fewer matches than expected                                       â”‚
â”‚    â€¢ More outliers/bad matches                                         â”‚
â”‚    â€¢ Gradual drift or sudden jumps                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### D435i Auto-Exposure

The RealSense camera adjusts exposure automatically, which can cause tracking degradation:

```bash
# Check current exposure
ros2 topic echo /camera/camera/color/camera_info --once

# Monitor tracking during lighting changes
ros2 topic echo /rtabmap/odom_info --field lost --field inliers
```

### Expected Behavior - Lighting Change Test

**Scenario**: Point camera at window, then pan away (auto-exposure triggers)

```
Time     Lost    Features  Matches  Inliers   Lighting Status
----------------------------------------------------------------------
   1s    false        905      540      118   Normal room light
   2s    false        898      525      112   Normal
   3s    false        892      518      105   Panning toward window
   4s    false        756      312       68   Auto-exposure adjusting
   5s    false        684      245       42   Bright â†’ darker transition
   6s    true         423       89        0   LOST! Descriptors changed
   7s    true         512      124       12   LOW - recovering
   8s    false        834      456       78   Stabilized at new exposure
----------------------------------------------------------------------

ANALYSIS:
  - During exposure transition (frames 4-7), tracking degraded
  - Inliers dropped: 118 â†’ 42 â†’ 0 â†’ 12 â†’ 78
  - Recovery took ~2-3 seconds after exposure stabilized
```

**The problem**: Same physical features have different pixel values after exposure change, causing descriptor mismatches!

![Auto-exposure changes pixel values, breaking feature descriptor matching](auto-exposure-problem.png){.lightbox fig-alt="Same feature with different descriptors after lighting change"}

### Eureka Moment #10

::: {.callout-tip}
## Vision Is Affected by Lighting

**Why it happens:**
- Feature descriptors encode pixel values
- Lighting changes alter pixel values
- Same physical feature â†’ different descriptor
- Matching confidence drops

**What IMU provides:**
- IMU measures acceleration and rotation
- Completely independent of lighting
- Works in complete darkness!

**This matters for robots:**
- Day/night transitions
- Indoor/outdoor transitions
- Moving shadows
- Flickering lights
:::

---

## The Opposite Weaknesses Pattern

Now we can see the beautiful symmetry between IMU and vision problems:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IMU vs VISION: OPPOSITE WEAKNESSES                         â”‚
â”‚                                                                         â”‚
â”‚                        IMU                        VISION                â”‚
â”‚                        â”€â”€â”€                        â”€â”€â”€â”€â”€â”€                â”‚
â”‚                                                                         â”‚
â”‚    Fast Motion:        âœ… Excellent               âŒ Fails (blur)      â”‚
â”‚                        (400 Hz sampling)          (feature loss)       â”‚
â”‚                                                                         â”‚
â”‚    Slow Motion:        âš ï¸ Drifts                  âœ… Excellent         â”‚
â”‚                        (integration error)        (clear features)     â”‚
â”‚                                                                         â”‚
â”‚    Textureless:        âœ… Works                   âŒ Fails             â”‚
â”‚                        (physics-based)            (no features)        â”‚
â”‚                                                                         â”‚
â”‚    Darkness:           âœ… Works                   âŒ Fails             â”‚
â”‚                        (no light needed)          (camera blind)       â”‚
â”‚                                                                         â”‚
â”‚    Long Duration:      âŒ Fails                   âœ… Loop closure      â”‚
â”‚                        (unbounded drift)          (corrects drift)     â”‚
â”‚                                                                         â”‚
â”‚    Absolute Yaw:       âŒ No reference            âœ… Map-relative      â”‚
â”‚                        (needs magnetometer)       (visual landmarks)   â”‚
â”‚                                                                         â”‚
â”‚                                                                         â”‚
â”‚           These are COMPLEMENTARY failure modes!                       â”‚
â”‚           Fusion can leverage the strengths of both!                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![IMU and Vision have complementary failure modes - perfect for sensor fusion!](imu-vs-vision-complementary.png){.lightbox fig-alt="IMU vs Vision complementary weaknesses comparison chart"}

### Comparison Table

| Scenario | IMU Alone | Vision Alone | Fusion (VIO) |
|----------|-----------|--------------|--------------|
| Stationary | âœ… OK (filtered) | âœ… OK | âœ… OK |
| Slow motion | âš ï¸ Drifts | âœ… Excellent | âœ… Excellent |
| **Fast motion** | âœ… OK | âŒ **Fails** | âœ… **IMU rescues** |
| **Textureless** | âœ… OK (short term) | âŒ **Fails** | âœ… **IMU bridges** |
| **Darkness** | âœ… OK | âŒ **Fails** | âš ï¸ IMU only |
| Long duration | âŒ Fails | âš ï¸ Loop closure helps | âœ… Best of both |

---

## The Math Doesn't Work: Why We Need Both

### IMU Integration Grows Error

```
Position error from IMU âˆ tÂ²

With 0.01 m/sÂ² bias:
â€¢ After 10s: ~0.5m error
â€¢ After 60s: ~18m error
â€¢ After 5 min: ~450m error!

IMU needs periodic "corrections" from an absolute source.
```

### Vision Needs Continuous Features

```
Visual odometry gap = no features for N frames

If camera sees blank wall for 1 second (30 frames):
â€¢ No feature matches possible
â€¢ Odometry output: NOTHING or WRONG
â€¢ Robot has no idea where it went

Vision needs something to "fill the gaps" during feature-less periods.
```

### The Solution Preview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHY FUSION WORKS                                     â”‚
â”‚                                                                         â”‚
â”‚    Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º      â”‚
â”‚                                                                         â”‚
â”‚    IMU:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        â”‚
â”‚            Always available, but drifting over time                    â”‚
â”‚                                                                         â”‚
â”‚    Vision: â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“          â”‚
â”‚            Sometimes lost (fast motion, textureless)                   â”‚
â”‚                                                                         â”‚
â”‚    VIO:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        â”‚
â”‚            IMU: continuous prediction                                   â”‚
â”‚            Vision: periodic correction                                  â”‚
â”‚            Result: STABLE, CONTINUOUS pose estimation!                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary: Vision-Only Failures

![Three failure modes of visual odometry: fast motion, textureless surfaces, and lighting changes](visual-odometry-failure-modes.png){.lightbox fig-alt="Visual odometry failure modes diagram"}

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PART 2 SUMMARY: WHY VISION ALONE FAILS                     â”‚
â”‚                                                                         â”‚
â”‚   Problem #8: Fast motion causes feature blur                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   Symptom: Tracking lost during rapid movement                         â”‚
â”‚   Impact: Robot loses localization when moving quickly                 â”‚
â”‚                                                                         â”‚
â”‚   Problem #9: Textureless surfaces have no features                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   Symptom: Tracking fails on blank walls, ceilings                     â”‚
â”‚   Impact: Robot can't navigate corridors, warehouses                   â”‚
â”‚                                                                         â”‚
â”‚   Problem #10: Lighting changes affect matching                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   Symptom: Drift when exposure changes                                 â”‚
â”‚   Impact: Day/night, indoor/outdoor transitions fail                   â”‚
â”‚                                                                         â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                  â”‚
â”‚   KEY INSIGHT: Vision and IMU have OPPOSITE failures!                  â”‚
â”‚   This is what makes FUSION so powerful!                               â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What's Next: The Solution!

We've now experienced both:

1. **Part 1**: IMU alone â†’ drift, yaw problems
2. **Part 2**: Vision alone â†’ fast motion, textureless failures

In **Part 3**, we'll combine them into **Visual-Inertial Odometry (VIO)**:

- IMU provides high-frequency motion estimates
- Vision corrects drift periodically
- Result: Robust pose estimation that handles both failure modes!

::: {.callout-tip}
## The Preview
Part 3 experiments:
- **Experiment 11**: IMU rescues fast motion
- **Experiment 12**: Vision corrects IMU drift
- **Experiment 13**: Isaac ROS Visual SLAM with IMU
- **Experiment 14**: Complete VIO pipeline

We'll build **exactly what yDx.M + external sensors** achieves - with our D435i alone!
:::

---

## Preparation Checklist

Before Workshop 4, make sure you can:

- [ ] Launch RTAB-Map visual odometry
- [ ] Observe tracking loss during fast motion
- [ ] Understand why textureless surfaces fail
- [ ] Know the complementary nature of IMU and vision failures
- [ ] Articulate why fusion is needed

---

## About This Learning Journey

By experiencing these failures firsthand, you'll:

1. **Deeply understand** why sensor fusion exists
2. **Appreciate** what yDx.M + external sensors achieve
3. **Know when** each sensor type is reliable
4. **Debug** fusion problems by understanding individual sensor limits

The workshop will show professional solutions - we're building the foundation to understand them!

---

## Resources

- [RTAB-Map Documentation](http://wiki.ros.org/rtabmap_ros)
- [Visual Odometry Fundamentals](https://www.iro.umontreal.ca/~labMDR/papers/visual_odometry_tutorial.pdf)
- [Feature Detection Overview](https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html)
- [Why VIO Works](https://arxiv.org/abs/1906.06097) (Academic Paper)
