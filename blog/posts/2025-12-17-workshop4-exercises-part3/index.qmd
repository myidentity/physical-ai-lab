---
title: "Workshop 4 Preview: Fusion - The Best of Both Worlds"
subtitle: "Part 3 of 4: Building Robust Visual-Inertial Odometry with the D435i"
author: "Rajesh"
date: "2025-12-17T14:00:00"
categories: [ros2, vio, slam, perception, workshop, roscon-india, realsense, d435i, sensor-fusion]
image: "thumbnail.png"
toc: true
toc-depth: 3
code-fold: true
code-summary: "Show code"
lightbox: true
---

## The Journey So Far

| Part | What We Learned |
|------|-----------------|
| [Part 1](../2025-12-17-workshop4-exercises-part1/) | IMU alone: drifts, no absolute yaw, position explodes |
| [Part 2](../2025-12-17-workshop4-exercises-part2/) | Vision alone: fails on fast motion, textureless areas |

**The key insight**: IMU and vision have **opposite** failure modes!

Now we combine them to get the **best of both worlds**.

---

## What Fusion Achieves

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    VISUAL-INERTIAL ODOMETRY (VIO)                       │
│                                                                         │
│    ┌───────────────────────────────────────────────────────────────┐   │
│    │                      THE FUSION MAGIC                          │   │
│    │                                                                │   │
│    │   IMU @ 400 Hz                   Vision @ 30 Hz               │   │
│    │   ┌─────────────┐               ┌─────────────┐               │   │
│    │   │ High-rate   │               │ Absolute    │               │   │
│    │   │ motion      │               │ position    │               │   │
│    │   │ prediction  │               │ correction  │               │   │
│    │   └──────┬──────┘               └──────┬──────┘               │   │
│    │          │                             │                       │   │
│    │          └─────────────┬───────────────┘                       │   │
│    │                        │                                       │   │
│    │                        ▼                                       │   │
│    │              ┌─────────────────┐                               │   │
│    │              │  FUSION FILTER  │                               │   │
│    │              │  (EKF / UKF)    │                               │   │
│    │              └────────┬────────┘                               │   │
│    │                       │                                        │   │
│    │                       ▼                                        │   │
│    │              ┌─────────────────┐                               │   │
│    │              │ ROBUST POSE     │                               │   │
│    │              │ Fast motion: OK │                               │   │
│    │              │ Textureless: OK │                               │   │
│    │              │ Long term: OK   │                               │   │
│    │              └─────────────────┘                               │   │
│    │                                                                │   │
│    └────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Experiment 11: IMU Rescues Fast Motion

### The Setup

We'll run RTAB-Map with IMU fusion enabled and test the same fast motion that broke vision-only odometry.

### Launch Commands

```bash
# Terminal 1: RealSense with IMU
ros2 launch realsense2_camera rs_launch.py \
    enable_gyro:=true \
    enable_accel:=true \
    unite_imu_method:=2 \
    align_depth.enable:=true

# Terminal 2: Run Madgwick filter (required for orientation)
ros2 run imu_filter_madgwick imu_filter_madgwick_node --ros-args \
    -r imu/data_raw:=/camera/camera/imu \
    -p use_mag:=false \
    -p world_frame:=enu

# Terminal 3: RTAB-Map with IMU fusion enabled!
ros2 launch rtabmap_launch rtabmap.launch.py \
    args:="--delete_db_on_start" \
    rgb_topic:=/camera/camera/color/image_raw \
    depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
    camera_info_topic:=/camera/camera/color/camera_info \
    frame_id:=camera_link \
    approx_sync:=true \
    visual_odometry:=true \
    imu_topic:=/imu/data \
    wait_imu_to_init:=true
```

### The Test

```bash
# Terminal 4: Monitor odometry
ros2 topic hz /odom

# Perform the same fast motion test from Part 2
# Shake camera rapidly for 2 seconds
```

### What Happens Now

**Part 2 (Vision Only):**
```
Fast shake → WARNING: Visual odometry lost!
           → Odometry: NO OUTPUT or JUMPS
           → Position estimate: WRONG
```

**Part 3 (With IMU Fusion):**
```
Fast shake → IMU pre-integration maintains motion estimate
           → Visual odometry recovers when motion slows
           → Position estimate: CONTINUOUS and STABLE!
```

### The IMU Pre-Integration Concept

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    IMU PRE-INTEGRATION                                  │
│                                                                         │
│    Time ──────────────────────────────────────────────────────────►    │
│                                                                         │
│    Vision:   [frame]     [blur]  [blur]  [blur]     [frame]            │
│               t=0        t=33ms  t=66ms  t=100ms    t=133ms            │
│               pose₀       ???     ???     ???       pose₁              │
│                                                                         │
│    IMU:      │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │             │
│              Every 2.5ms (400 Hz)                                      │
│                                                                         │
│    Pre-integration:                                                    │
│              ┌─────────────────────────────────────────┐               │
│              │ Integrate all IMU samples between       │               │
│              │ vision frames to get:                   │               │
│              │   Δposition, Δvelocity, Δorientation   │               │
│              └─────────────────────────────────────────┘               │
│                                                                         │
│    Result:   Even when vision fails, we know how camera moved!         │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Eureka Moment #11

::: {.callout-tip}
## IMU Bridges Visual Gaps!

**Why it works:**
1. IMU samples at 400 Hz (vs vision at 30 Hz)
2. During motion blur, IMU still measures motion
3. Pre-integration accumulates IMU into relative pose change
4. When vision recovers, fusion filter corrects any drift

**Practical impact:**
- Robot can handle bumps, vibrations, rapid turns
- No more "tracking lost" during normal operation
- Essential for drones, mobile robots, handheld devices
:::

---

## Experiment 12: Visual Odometry Corrects IMU Drift

### The Problem from Part 1

Remember: IMU alone drifts ~18 meters in 60 seconds due to accelerometer bias.

### The Fusion Solution

```bash
# Configure robot_localization to fuse both IMU and visual odometry

cat > /tmp/vio_fusion.yaml << 'EOF'
ekf_filter_node:
  ros__parameters:
    frequency: 50.0
    sensor_timeout: 0.1
    two_d_mode: false

    map_frame: map
    odom_frame: odom
    base_link_frame: camera_link
    world_frame: odom

    # Visual odometry (from RTAB-Map)
    odom0: /rtabmap/odom
    odom0_config: [true,  true,  true,    # x, y, z position
                   true,  true,  true,    # roll, pitch, yaw
                   false, false, false,   # vx, vy, vz
                   false, false, false,   # vroll, vpitch, vyaw
                   false, false, false]   # ax, ay, az
    odom0_differential: false

    # IMU (filtered)
    imu0: /imu/data
    imu0_config: [false, false, false,    # Don't use IMU position
                  true,  true,  true,     # Use orientation
                  false, false, false,    # Don't use velocity
                  true,  true,  true,     # Use angular velocity
                  true,  true,  true]     # Use acceleration
    imu0_differential: false
    imu0_remove_gravitational_acceleration: true
EOF

# Run the fusion filter
ros2 run robot_localization ekf_node --ros-args \
    --params-file /tmp/vio_fusion.yaml
```

### The Test

```bash
# Monitor the fused output
ros2 topic echo /odometry/filtered --field pose.pose.position

# Leave camera stationary for 60 seconds
# With IMU-only: would drift meters
# With VIO fusion: should stay near zero!
```

### Results Comparison

| Time | IMU Only Position | VIO Fusion Position |
|------|-------------------|---------------------|
| 0s | (0, 0, 0) | (0, 0, 0) |
| 10s | (0.5, -0.3, 0.1) | (0.01, -0.01, 0.00) |
| 30s | (4.5, -2.1, 0.8) | (0.02, -0.02, 0.01) |
| 60s | (18.3, -8.7, 3.2) | (0.03, -0.02, 0.01) |

### The Correction Mechanism

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    DRIFT CORRECTION VIA VISION                          │
│                                                                         │
│    Position Error ──────────────────────────────────────────────►      │
│                                                                         │
│    IMU Only:                                                           │
│    ┌──────────────────────────────────────────────────────────────┐   │
│    │                                                        ↗ 18m │   │
│    │                                               ↗             │   │
│    │                                    ↗                        │   │
│    │                          ↗                                  │   │
│    │                ↗                                            │   │
│    │      ↗                                                      │   │
│    │↗                                                            │   │
│    └──────────────────────────────────────────────────────────────┘   │
│                                                                         │
│    VIO Fusion:                                                         │
│    ┌──────────────────────────────────────────────────────────────┐   │
│    │ → → → ↓ → → → ↓ → → → ↓ → → → ↓ → → → ↓ → → → ↓ ~0.03m    │   │
│    │      vision    vision    vision    vision    vision         │   │
│    │      correction correction                                  │   │
│    └──────────────────────────────────────────────────────────────┘   │
│                                                                         │
│    Vision provides periodic "resets" that prevent drift accumulation!  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Eureka Moment #12

::: {.callout-tip}
## Vision Corrects IMU Drift!

**The fusion dynamics:**
1. IMU provides high-frequency pose prediction (400 Hz)
2. Visual odometry provides lower-frequency corrections (30 Hz)
3. EKF optimally weights both based on covariances
4. Result: drift is **bounded** instead of **unbounded**

**Why this matters:**
- Long-duration operation becomes possible
- No need for expensive "tactical grade" IMUs
- Consumer-grade sensors achieve professional results
:::

---

## Experiment 13: Isaac ROS Visual SLAM with IMU

### GPU-Accelerated VIO

NVIDIA's Isaac ROS Visual SLAM provides hardware-accelerated VIO. Let's test it!

```bash
# Check if Isaac ROS is available in the container
ros2 pkg list | grep isaac

# If available, launch Isaac ROS Visual SLAM
ros2 launch isaac_ros_visual_slam isaac_ros_visual_slam_realsense.launch.py \
    enable_imu_fusion:=true
```

### Performance Comparison

| Metric | RTAB-Map VIO | Isaac ROS VSLAM |
|--------|--------------|-----------------|
| CPU Usage | ~40% | ~15% |
| GPU Usage | 0% | ~30% |
| Latency | ~33ms | ~10ms |
| Max Speed | Limited by CPU | Hardware accelerated |

### When to Use Which

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    CHOOSING YOUR VIO STACK                              │
│                                                                         │
│    RTAB-Map VIO                      Isaac ROS Visual SLAM             │
│    ─────────────                     ────────────────────              │
│    ✅ Works everywhere               ✅ GPU accelerated                │
│    ✅ Loop closure & mapping         ✅ Low latency (<10ms)            │
│    ✅ No NVIDIA required             ✅ High speed handling            │
│    ⚠️ Higher CPU usage              ⚠️ Requires Jetson/NVIDIA         │
│    ⚠️ Slower (CPU-bound)            ⚠️ No built-in mapping            │
│                                                                         │
│    Best for:                         Best for:                         │
│    • Desktop development             • Production robots               │
│    • SLAM/mapping tasks              • Drones (latency-critical)       │
│    • Non-NVIDIA hardware             • Jetson-based systems            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Eureka Moment #13

::: {.callout-tip}
## GPU Acceleration Changes the Game

**Why Isaac ROS matters:**
- Feature extraction on GPU → faster
- VIO can keep up with faster motion
- Lower latency → better control loops
- CPU freed for other tasks

**Workshop relevance:**
- yDx.M focuses on the IMU side
- You still need a VIO stack for full solution
- Understanding both helps you architect systems
:::

---

## Experiment 14: Complete VIO Pipeline

### The Full Stack

Let's assemble everything into a complete, robust VIO pipeline:

```bash
# Terminal 1: RealSense (camera + IMU)
ros2 launch realsense2_camera rs_launch.py \
    enable_gyro:=true \
    enable_accel:=true \
    unite_imu_method:=2 \
    align_depth.enable:=true \
    initial_reset:=true

# Terminal 2: IMU Filter (raw → orientation)
ros2 run imu_filter_madgwick imu_filter_madgwick_node --ros-args \
    -r imu/data_raw:=/camera/camera/imu \
    -p use_mag:=false \
    -p world_frame:=enu \
    -p publish_tf:=false

# Terminal 3: RTAB-Map (VIO + Mapping)
ros2 launch rtabmap_launch rtabmap.launch.py \
    args:="--delete_db_on_start" \
    rgb_topic:=/camera/camera/color/image_raw \
    depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
    camera_info_topic:=/camera/camera/color/camera_info \
    frame_id:=camera_link \
    visual_odometry:=true \
    imu_topic:=/imu/data \
    wait_imu_to_init:=true \
    approx_sync:=true

# Terminal 4: RViz2 for visualization
rviz2
```

### Test All Scenarios

Now let's verify our VIO handles all the failure modes:

| Test | How | Expected Result |
|------|-----|-----------------|
| **Normal motion** | Move slowly | Smooth tracking |
| **Fast motion** | Shake rapidly | Continues tracking! |
| **Textureless** | Point at wall | IMU bridges, recovers |
| **Stationary** | Leave still | No drift |
| **Long duration** | Run 5 minutes | Bounded drift |

### Recording a Test Rosbag

```bash
# Record synchronized data for analysis
ros2 bag record \
    /camera/camera/color/image_raw \
    /camera/camera/aligned_depth_to_color/image_raw \
    /camera/camera/imu \
    /imu/data \
    /rtabmap/odom \
    -o vio_test_rosbag
```

### System Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    COMPLETE D435i VIO PIPELINE                          │
│                                                                         │
│    ┌─────────────────────────────────────────────────────────────────┐ │
│    │                     Intel RealSense D435i                        │ │
│    │   ┌──────────┐    ┌──────────┐    ┌──────────┐                 │ │
│    │   │   RGB    │    │  Depth   │    │   IMU    │                 │ │
│    │   │ 30 Hz    │    │  30 Hz   │    │  400 Hz  │                 │ │
│    │   └────┬─────┘    └────┬─────┘    └────┬─────┘                 │ │
│    └────────┼───────────────┼───────────────┼───────────────────────┘ │
│             │               │               │                          │
│             ▼               ▼               ▼                          │
│    ┌────────────────────────────┐   ┌───────────────┐                 │
│    │      realsense_camera      │   │   Madgwick    │                 │
│    │         ROS 2 Node         │   │    Filter     │                 │
│    └────────────┬───────────────┘   └───────┬───────┘                 │
│                 │                           │                          │
│                 │    /color/image_raw       │ /imu/data               │
│                 │    /depth/image_raw       │ (with orientation)      │
│                 ▼                           ▼                          │
│    ┌──────────────────────────────────────────────────────────────┐   │
│    │                        RTAB-Map                               │   │
│    │  ┌─────────────────┐    ┌─────────────────┐                  │   │
│    │  │ Visual Odometry │◄───│ IMU Integration │                  │   │
│    │  │  (Feature Match)│    │ (Pre-integrate) │                  │   │
│    │  └────────┬────────┘    └────────┬────────┘                  │   │
│    │           └──────────┬───────────┘                           │   │
│    │                      ▼                                        │   │
│    │            ┌─────────────────┐                               │   │
│    │            │  Graph SLAM +   │                               │   │
│    │            │  Loop Closure   │                               │   │
│    │            └────────┬────────┘                               │   │
│    └─────────────────────┼────────────────────────────────────────┘   │
│                          │                                             │
│                          ▼                                             │
│              ┌──────────────────────┐                                 │
│              │     /rtabmap/odom    │                                 │
│              │  Robust VIO Output!  │                                 │
│              └──────────────────────┘                                 │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Eureka Moment #14

::: {.callout-tip}
## We Built What yDx.M + External Sensors Achieves!

**Our D435i VIO pipeline:**
- D435i IMU (6-DOF) → yDx.M equivalent (minus magnetometer)
- D435i RGB-D → External camera equivalent
- Madgwick filter → AHRS equivalent
- RTAB-Map VIO → Sensor fusion equivalent

**What we learned:**
1. Raw sensors need processing (filters, calibration)
2. Single sensors have fundamental limits
3. Fusion combines strengths, masks weaknesses
4. Architecture matters (timing, covariances, frames)

**What yDx.M adds:**
- Factory calibration (no manual calibration)
- Magnetometer option (absolute yaw)
- Distributed sensing (redundancy)
- Production-grade reliability
:::

---

## Performance Comparison Matrix

After all our experiments, here's what we learned:

| Scenario | IMU Only | Vision Only | VIO Fusion |
|----------|----------|-------------|------------|
| **Stationary** | ✅ OK | ✅ OK | ✅ OK |
| **Slow motion** | ⚠️ Drifts | ✅ Excellent | ✅ Excellent |
| **Fast motion** | ✅ OK | ❌ Fails | ✅ **Excellent** |
| **Textureless** | ✅ OK (short) | ❌ Fails | ✅ **Good** |
| **Darkness** | ✅ OK | ❌ Fails | ⚠️ IMU only mode |
| **Long duration** | ❌ Fails | ⚠️ Needs loop closure | ✅ **Best of both** |
| **Absolute yaw** | ❌ Drifts | ✅ Map-relative | ✅ **Map-relative** |

---

## Summary: The Fusion Achievement

```
┌─────────────────────────────────────────────────────────────────────────┐
│              PART 3 SUMMARY: THE POWER OF FUSION                        │
│                                                                         │
│   Achievement #11: IMU bridges motion blur gaps                        │
│   ────────────────────────────────────────────────                     │
│   IMU at 400 Hz keeps tracking when vision fails                       │
│   Pre-integration accumulates motion between frames                    │
│                                                                         │
│   Achievement #12: Vision corrects IMU drift                           │
│   ────────────────────────────────────────────────                     │
│   Periodic visual corrections bound IMU drift                          │
│   From 18m drift to ~0.03m drift over 60 seconds                      │
│                                                                         │
│   Achievement #13: GPU acceleration is available                       │
│   ────────────────────────────────────────────────                     │
│   Isaac ROS VSLAM: 10ms latency, lower CPU usage                      │
│   Choose stack based on hardware and requirements                      │
│                                                                         │
│   Achievement #14: Complete VIO pipeline works!                        │
│   ────────────────────────────────────────────────                     │
│   D435i alone provides robust odometry                                 │
│   Foundation for understanding yDx.M integration                       │
│                                                                         │
│   ═══════════════════════════════════════════════════                  │
│   KEY TAKEAWAY: Fusion > Sum of Parts                                  │
│   Understanding failures helps us appreciate solutions!                │
│   ═══════════════════════════════════════════════════                  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## What's Next: Workshop Context

In **Part 4**, we'll focus on workshop preparation:

- Yaanendriya company background
- yDx.M module specifications
- How workshop exercises map to what we learned
- Questions to ask the presenters
- What yDx.M adds beyond our D435i solution

::: {.callout-note}
## The Workshop Will Teach
- yDx.M ROS 2 driver usage
- Their fusion algorithms
- Distributed sensor networks
- Production-grade calibration
- Real-world deployment patterns

We've built the **conceptual foundation** - the workshop provides the **professional implementation**!
:::

---

## Preparation Checklist

Before Workshop 4, make sure you can:

- [ ] Run RTAB-Map with IMU fusion enabled
- [ ] Demonstrate IMU rescuing fast motion
- [ ] Show vision correcting IMU drift
- [ ] Explain the complementary nature of sensor failures
- [ ] Articulate why VIO is more robust than either sensor alone
- [ ] Record and replay rosbags with synchronized sensor data

---

## Resources

- [RTAB-Map IMU Integration](http://wiki.ros.org/rtabmap_ros/Tutorials/HandHeldMapping)
- [robot_localization Documentation](http://docs.ros.org/en/melodic/api/robot_localization/html/)
- [Isaac ROS Visual SLAM](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam)
- [Visual-Inertial Odometry Overview](https://rpg.ifi.uzh.ch/docs/TRO16_forster.pdf)
- [EKF Sensor Fusion Tutorial](https://automaticaddison.com/sensor-fusion-using-the-robot-localization-package-ros-2/)
